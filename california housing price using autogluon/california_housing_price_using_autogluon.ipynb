{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install tools\n",
        "!pip install -q kaggle autogluon.tabular scikit-learn\n",
        "\n",
        "# Upload your Kaggle key (kaggle.json) from your computer\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # pick kaggle.json\n",
        "\n",
        "# Put the key where Kaggle CLI expects it\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Quick check\n",
        "!kaggle --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "g2UvNVtirJOb",
        "outputId": "be6bcfc7-24c4-47d2-b2e2-92fa2d0c7e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0ebf412f-e7d8-475c-be29-f586ebf18d34\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0ebf412f-e7d8-475c-be29-f586ebf18d34\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pandas as pd\n",
        "\n",
        "COMP = \"california-homelessness-prediction-challenge\"\n",
        "DATA_DIR = \"/content/data\"\n",
        "DATASET_DIR = os.path.join(DATA_DIR, COMP)\n",
        "MODELS_DIR = os.path.join(DATA_DIR, \"AutoGluonModels\")\n",
        "\n",
        "# Make folders and pull files\n",
        "!mkdir -p \"{DATA_DIR}\" \"{DATASET_DIR}\" \"{MODELS_DIR}\"\n",
        "\n",
        "# (A) See what files exist in the competition (helps sanity-check)\n",
        "!kaggle competitions files -c \"{COMP}\"\n",
        "\n",
        "# (B) Download + unzip\n",
        "!kaggle competitions download -c \"{COMP}\" -p \"{DATA_DIR}\" --force\n",
        "!unzip -o -q \"{DATA_DIR}/{COMP}.zip\" -d \"{DATASET_DIR}\"\n",
        "!rm -f \"{DATA_DIR}/{COMP}.zip\"\n",
        "\n",
        "# Peek at the files we got\n",
        "print(\"Files:\", os.listdir(DATASET_DIR))\n",
        "\n",
        "# Load sample_submission to detect the target column(s)\n",
        "sub_path = os.path.join(DATASET_DIR, \"sample_submission.csv\")\n",
        "sub = pd.read_csv(sub_path)\n",
        "print(\"sample_submission columns:\", list(sub.columns))\n",
        "\n",
        "# Heuristics to auto-detect id + target(s)\n",
        "# - If there are exactly 2 columns, it's usually [id_col, target_col]\n",
        "# - If more, we assume the first column is an ID-like column\n",
        "id_col = sub.columns[0]\n",
        "target_cols = [c for c in sub.columns if c != id_col]\n",
        "print(\"ID column guess:\", id_col)\n",
        "print(\"Target column(s) guess:\", target_cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeUEGUyDrl5t",
        "outputId": "fb04592d-dc26-47e7-eace-310f96396564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name                          size  creationDate                \n",
            "----------------------  ----------  --------------------------  \n",
            "sample_submission.csv          634  2025-08-02 01:31:53.475000  \n",
            "sandbox_submission.csv         634  2025-08-02 01:31:53.475000  \n",
            "test.csv                     33559  2025-08-02 01:31:53.475000  \n",
            "train.csv                    79563  2025-08-02 01:31:53.475000  \n",
            "Downloading california-homelessness-prediction-challenge.zip to /content/data\n",
            "  0% 0.00/50.7k [00:00<?, ?B/s]\n",
            "100% 50.7k/50.7k [00:00<00:00, 158MB/s]\n",
            "Files: ['sample_submission.csv', 'test.csv', 'sandbox_submission.csv', 'train.csv']\n",
            "sample_submission columns: ['ID', 'HOMELESS_RATE']\n",
            "ID column guess: ID\n",
            "Target column(s) guess: ['HOMELESS_RATE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, random, time\n",
        "from autogluon.tabular import TabularPredictor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED); random.seed(SEED)\n",
        "\n",
        "train_path = os.path.join(DATASET_DIR, \"train.csv\")\n",
        "test_path  = os.path.join(DATASET_DIR, \"test.csv\")\n",
        "\n",
        "train_df = pd.read_csv(train_path, low_memory=False)\n",
        "test_df  = pd.read_csv(test_path,  low_memory=False)\n",
        "\n",
        "print(\"train shape:\", train_df.shape)\n",
        "print(\"test shape :\", test_df.shape)\n",
        "\n",
        "# Try to pick features = columns shared by train & test, excluding ID and target columns\n",
        "shared_cols = [c for c in train_df.columns if c in test_df.columns]\n",
        "if id_col in shared_cols:\n",
        "    shared_cols.remove(id_col)\n",
        "for tc in target_cols:\n",
        "    if tc in shared_cols:\n",
        "        shared_cols.remove(tc)\n",
        "\n",
        "print(\"Feature columns (first 10):\", shared_cols[:10])\n",
        "\n",
        "# We’ll handle only the FIRST target for a simple baseline\n",
        "TARGET = target_cols[0]\n",
        "print(\"Using target:\", TARGET)\n",
        "\n",
        "# Optional: drop rows with missing target\n",
        "train_df = train_df.dropna(subset=[TARGET]).reset_index(drop=True)\n",
        "\n",
        "# Simple split\n",
        "train_part, dev_part = train_test_split(train_df, test_size=0.2, random_state=SEED)\n",
        "\n",
        "# Train a quick baseline (good_quality for speed)\n",
        "save_dir = os.path.join(MODELS_DIR, f\"baseline_{time.strftime('%Y%m%d_%H%M%S')}\")\n",
        "predictor = TabularPredictor(label=TARGET, path=save_dir, eval_metric=\"rmse\", verbosity=2)\n",
        "predictor.fit(\n",
        "    train_data=train_part[[*shared_cols, TARGET]],\n",
        "    tuning_data=dev_part[[*shared_cols, TARGET]],\n",
        "    presets=\"good_quality\",\n",
        "    time_limit=300,              # ~5 minutes cap\n",
        "    num_bag_folds=0,\n",
        "    num_stack_levels=0,\n",
        "    keep_only_best=True,\n",
        ")\n",
        "\n",
        "# Evaluate on dev set\n",
        "dev_metrics = predictor.evaluate(dev_part[[*shared_cols, TARGET]])\n",
        "print(\"Dev metrics:\", dev_metrics)\n",
        "\n",
        "# Build submission using sample_submission’s exact columns/order\n",
        "submission = sub.copy()\n",
        "preds = predictor.predict(test_df[shared_cols])\n",
        "\n",
        "# If the competition expects integers, round safely; else keep floats.\n",
        "# We’ll keep floats by default:\n",
        "submission[TARGET] = preds.values\n",
        "\n",
        "# If sample_submission had multiple target columns, we’d need to predict each.\n",
        "# For now, we handle the single-column case. If there are more, tell me and I’ll extend this cell.\n",
        "\n",
        "out_csv = os.path.join(save_dir, \"submission.csv\")\n",
        "submission.to_csv(out_csv, index=False)\n",
        "print(\"Saved submission to:\", out_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqnZwgjXrr0o",
        "outputId": "2b648aa0-c82f-4b4d-cc5c-4b4e72f31629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Memory Avail:       11.26 GB / 12.67 GB (88.8%)\n",
            "Disk Space Avail:   186.08 GB / 225.83 GB (82.4%)\n",
            "===================================================\n",
            "Presets specified: ['good_quality']\n",
            "Using hyperparameters preset: hyperparameters='light'\n",
            "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=0, num_bag_sets=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: (130, 33)\n",
            "test shape : (56, 32)\n",
            "Feature columns (first 10): ['AGE_U18_PCT', 'AGE_18_24_PCT', 'AGE_25_34_PCT', 'AGE_35_44_PCT', 'AGE_45_54_PCT', 'AGE_55_59_PCT', 'AGE_60_61_PCT', 'AGE_62_64_PCT', 'AGE_65_69_PCT', 'AGE_70_79_PCT']\n",
            "Using target: HOMELESS_RATE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Beginning AutoGluon training ... Time limit = 300s\n",
            "AutoGluon will save models to \"/content/data/AutoGluonModels/baseline_20251025_184346\"\n",
            "Train Data Rows:    104\n",
            "Train Data Columns: 31\n",
            "Tuning Data Rows:    26\n",
            "Tuning Data Columns: 31\n",
            "Label Column:       HOMELESS_RATE\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (0.0587983619583155, 0.0, 0.00408, 0.0073)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11476.11 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 2): ['FAMILY_MEMBERS_UNDER_18_PCT', 'INDIVIDUALS_NOT_IN_FAMILY_UNITS_PCT']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 2 | ['FAMILY_MEMBERS_UNDER_18_PCT', 'INDIVIDUALS_NOT_IN_FAMILY_UNITS_PCT']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 29 | ['AGE_U18_PCT', 'AGE_18_24_PCT', 'AGE_25_34_PCT', 'AGE_35_44_PCT', 'AGE_45_54_PCT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 29 | ['AGE_U18_PCT', 'AGE_18_24_PCT', 'AGE_25_34_PCT', 'AGE_35_44_PCT', 'AGE_45_54_PCT', ...]\n",
            "\t0.6s = Fit runtime\n",
            "\t29 features in original data used to generate 29 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.68s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'FASTAI': [{}],\n",
            "\t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT ... Training model for up to 299.32s of the 299.32s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/10.8 GB\n",
            "\t-0.0032\t = Validation score   (-root_mean_squared_error)\n",
            "\t16.21s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 283.11s of the 283.10s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/10.7 GB\n",
            "\t-0.0031\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.28s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 282.82s of the 282.81s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t-0.007\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.18s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 281.51s of the 281.50s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\tWarning: Exception caused CatBoost to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 281.27s of the 281.27s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t-0.0043\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.7s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 280.45s of the 280.45s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/10.7 GB\n",
            "No improvement since epoch 6: early stopping\n",
            "\t-0.0028\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.56s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 277.85s of the 277.84s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\t-0.0034\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.79s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 277.04s of the 277.03s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/10.7 GB\n",
            "\t-0.0032\t = Validation score   (-root_mean_squared_error)\n",
            "\t8.72s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 268.29s of the 268.28s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/10.7 GB\n",
            "\t-0.0031\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.46s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.32s of the 267.81s of remaining time.\n",
            "\tEnsemble Weights: {'NeuralNetFastAI': 0.417, 'NeuralNetTorch': 0.333, 'XGBoost': 0.167, 'LightGBMLarge': 0.083}\n",
            "\t-0.0026\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 32.26s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 577.5 rows/s (26 batch size)\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: NeuralNetFastAI_FULL ...\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/10.7 GB\n",
            "\tStopping at the best epoch learned earlier - 6.\n",
            "\t0.35s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: XGBoost_FULL ...\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\t0.02s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: NeuralNetTorch_FULL ...\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/10.7 GB\n",
            "\t0.92s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMLarge_FULL ...\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/10.7 GB\n",
            "\t0.31s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'NeuralNetFastAI': 0.417, 'NeuralNetTorch': 0.333, 'XGBoost': 0.167, 'LightGBMLarge': 0.083}\n",
            "\t0.01s\t = Training   runtime\n",
            "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 1.72s ... Best model: \"WeightedEnsemble_L2_FULL\"\n",
            "Deleting model LightGBMXT. All files under /content/data/AutoGluonModels/baseline_20251025_184346/models/LightGBMXT will be removed.\n",
            "Deleting model LightGBM. All files under /content/data/AutoGluonModels/baseline_20251025_184346/models/LightGBM will be removed.\n",
            "Deleting model RandomForestMSE. All files under /content/data/AutoGluonModels/baseline_20251025_184346/models/RandomForestMSE will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under /content/data/AutoGluonModels/baseline_20251025_184346/models/ExtraTreesMSE will be removed.\n",
            "Deleting model NeuralNetFastAI. All files under /content/data/AutoGluonModels/baseline_20251025_184346/models/NeuralNetFastAI will be removed.\n",
            "Deleting model XGBoost. All files under /content/data/AutoGluonModels/baseline_20251025_184346/models/XGBoost will be removed.\n",
            "Deleting model NeuralNetTorch. All files under /content/data/AutoGluonModels/baseline_20251025_184346/models/NeuralNetTorch will be removed.\n",
            "Deleting model LightGBMLarge. All files under /content/data/AutoGluonModels/baseline_20251025_184346/models/LightGBMLarge will be removed.\n",
            "Deleting model WeightedEnsemble_L2. All files under /content/data/AutoGluonModels/baseline_20251025_184346/models/WeightedEnsemble_L2 will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/data/AutoGluonModels/baseline_20251025_184346\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev metrics: {'root_mean_squared_error': np.float64(-0.0017371703069817617), 'mean_squared_error': -3.0177606754591083e-06, 'mean_absolute_error': -0.0012172881570828155, 'r2': 0.6174983158683554, 'pearsonr': 0.7907787274275869, 'median_absolute_error': np.float64(-0.0008615442493178672)}\n",
            "Saved submission to: /content/data/AutoGluonModels/baseline_20251025_184346/submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SUBMIT_FILE = out_csv  # from the print above\n",
        "!kaggle competitions submit -c \"{COMP}\" -f \"{SUBMIT_FILE}\" -m \"baseline auto\"\n",
        "!kaggle competitions submissions -c \"{COMP}\" | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7NjHxTCr-rV",
        "outputId": "e9e5da1f-10ff-4c3f-e5b2-cdd0f854ecfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 1.05k/1.05k [00:00<00:00, 3.03kB/s]\n",
            "Successfully submitted to California Homelessness Prediction ChallengefileName        date                        description    status                    publicScore  privateScore  \n",
            "--------------  --------------------------  -------------  ------------------------  -----------  ------------  \n",
            "submission.csv  2025-10-25 18:44:47.563000  baseline auto  SubmissionStatus.PENDING                             \n"
          ]
        }
      ]
    }
  ]
}